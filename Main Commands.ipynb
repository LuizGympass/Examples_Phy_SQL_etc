{"cells":[{"cell_type":"markdown","source":["# PYSPARK"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"639da227-59c7-497b-a28b-38803db4972e"}}},{"cell_type":"markdown","source":["LIBRARIES\n\n- from pyspark.sql import SparkSession\n- from pyspark.sql.types import *\n- from pyspark.sql import functions as func\n- from pyspark.sql.functions import *\n- from pyspark.sql.functions import sum\n- from pyspark.sql.functions import expr\n- from pyspark.sql import SparkSession\n- from pyspark.sql import storagelevel\n- import panda as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1a1ff01-42c1-472a-8965-b84744f2f3ed"}}},{"cell_type":"markdown","source":["## RDD MANUPIPULATION"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b124d97-9251-4107-b06a-46f14a25db0a"}}},{"cell_type":"markdown","source":["- numeros = sc.parallelize([1,2,3,4,5,6,7,8,9,10]) # Criando um RDD\n\nBASIC OPERATIONS \n\n- numeros.take(5) # puxa os primeiros 5\n- numeros.top(5) # primeiros 5\n- numeros.collect() # Não aconselhavel, pode atrapalhar performance\n- numeros.count() \n- numeros.mean()\n- numeros.max()\n- numeros.min()\n- numeros.stdev() # Desvio padrão\n- numeros.filter(lambda filtro: filtro>8) # filtrando\n- amostra = numeros.sample(True,0.5,1) # Gerar amostrar, com ou sem reposição, probabilidade\n- numeros.map(lambda mapa: mapa * 2) # aplica ação em todos os elementos do RDD\n- numeros.union(numeros2) # Fez a união dos dois dados\n- numeros.subtract(numeros2)  # o que tem no numeros, mas não tem no numeros\n- compras.subtractByKey(debitos)\n- numeros.cartesian(numeros2) # Pegar o cartersiano \n\nMÉTODOS\n- compras.keys()\n- compras.values()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d86042ee-29f4-4314-a9d1-dc68314becce"}}},{"cell_type":"markdown","source":["## WORKING WITH DATA FRAMES\n\n- spark.read.csv(filename,header=False,schema=schema, sep=\";\")\n- spark.read.load(filename,header=False,format=\"csv\",inferschema=True)\n- df.schema\n- display(df)\n- df.show()\n- despachantes.select(\"ID\",\"Nome\",\"vendas\").where(Func.col(\"vendas\") > 20).show()\n- despachantes.select(\"ID\",\"Nome\",\"vendas\").where((Func.col(\"vendas\") > 20) & (Func.col(\"vendas\") < 40)).show() # Filtering with where \n- & = AND, | = OR, ~ = Not\n- novodf = despachantes.withColumnRenamed(\"nome\", \"nomes\")\n- despachantes2 = despachantes.withColumn(\"data2\",to_timestamp(Func.col(\"data\"),\"yyyy-mm-dd\"))\n- despachantes2.select(year(\"data\")).show() #consegue utilizar funcoes especificas de datas\n- despachantes2.select(year(\"data\")).distinct().show() #consegue utilizar funcoes especificas de datas\n- despachantes2.select(\"nome\",year(\"data\")).dist\n- despachantes2.select(\"nome\", year(\"data\")).orderBy(\"nome\").show() \n- despachantes2.select(\"data\").groupBy(year(\"data\")).count().show() \n- despachantes.select(Func.sum(\"vendas\")).show()\n- despachantes.orderBy(\"Vendas\").show()\n- despachantes.orderBy(Func.col(\"Vendas\").desc()).show() --> For using desc, it is mandatory use func.col functionality\n- despachantes.orderBy(Func.col(\"cidade\").desc(),Func.col(\"Vendas\").desc()).show() -- order by desc for more than one column\n- despachantes.groupBy(\"cidade\").agg(sum(\"vendas\")).show() -- Group by with sum IMPORTANTE MENTION THAT NEEDS TO USE AGG = AGREGATION\n- despachantes.groupBy(\"cidade\").agg(sum(\"vendas\")).orderBy(Func.col(\"sum(vendas)\").desc()).show() -- Group By + sum + order\n- despachantes.filter(Func.col(\"nome\") == \"Deolinda Vilela\").show()\n- df = pd.read.csv(filanme,sep=\";\") Transforming DF do pandas para o pyspark\n- spark = SparkSession.biolder.appName(\"Pandas\").getOrCreate()\n- df_spark = spark.createDataFrame(df_pandas)\n- pandas = df_spark.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cc11d48-3bd4-4da6-84d1-89e37b8abb91"}}},{"cell_type":"markdown","source":["## WORKING WITH DATABASES"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c3b333a-5dfe-4bb4-b87f-d0fb459fe27d"}}},{"cell_type":"markdown","source":["- spark.sql(\"show databases\").show()\n- spark.sql(\"create database desp\").show()\n- spark.sql(\"use desp\") # Set to database\n- despachantes.write.saveAsTable(\"despachantes\")\n- spark.sql(\"select * from despachantes\").show()\n- despachantes.write.mode(\"overwrite\").saveAsTable(\"Despachantes\")\n- despachantes = spark.sql(\"select * from Despachantes\")\n- spark.sql(\"show create table despachantes\").show(truncate=False)  -> SEE se é uma tabela externa, aparece a opçao Location\n- spark.catalog.listTables() -> based on the variable tableType = Managed or External is possible to identify whether is possible to manage this table or not\n- despachantes.createOrReplaceTempView(\"vw_despachantes\") - Temporary View, used only in this session\n- spark.sql(\"CREATE OR REPLACE VIEW DESP_VIEW AS SELECT * FROM DESPACHANTES\")\n- despachantes.createOrReplaceGlobalTempView(\"Globa_temp.despachantes\") -- Global view, could be used for everyone\n- spark.sql(\"CREATE OR REPLACE GLOBAL TEMP VIEW Global_DESP_VIEW AS SELECT * FROM DESPACHANTES\")\n- df.join(df1,df.id == df1.id,\"inner\").select(\"idrec\",\"datarec\",\"iddesp\")\n- df.join(df1,df.id == df1.id,\"left\").select(\"idrec\",\"datarec\",\"iddesp\")\n- df.join(df1,df.id == df1.id,\"right\").select(\"idrec\",\"datarec\",\"iddesp\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7303a01-273a-400b-8e68-a92f7e38c7c4"}}},{"cell_type":"markdown","source":["## SPARK STREAMING"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2bb1ab0-5d72-4aad-91c2-711dd3d308f6"}}},{"cell_type":"markdown","source":["- df = spark.readstream.json(\"path\", schema=vschema)\n- stcal = df.writestream.format(\"console\").outputmode(\"append\").trigger(processingtime=\"5 seconds\").option(\"checkpointlocation\").diretorio().start()\n- stcal.awaitTermination()\n\n- Streaming wriring in a postgree\ndef atualizapostgree(dataf, batchId)\n\n  dataf = spark.write.format(\"jdbc\").option(\"url\",\"jdbc:postgreesql://localhost?5432/vendas\").option(\"dbtables\",\"Vendas\").option(\"user\",\"postgress\").option(\"password\",\"123456\").option(\"driver\",örg.postgresql.Driver\").mode (\"append\").save()\n\nstcal.df.waitStream.foreachBatch(atualizapostgree).outpumode(\"append\").trigger(processingtime=\"5 seconds\").option(\"checkpointlocation\").diretorio().start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14e2d52d-4523-4557-859c-d44bfff4821f"}}},{"cell_type":"markdown","source":["## OPTIMIZATION"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39ed6e4c-4515-416d-8f1f-9deb67771094"}}},{"cell_type":"markdown","source":["- df.write.partitionBy(\"Column1\").saveAsTable(\"Tabela1\")\n- df.write.bucketBy(100,\"Column2\").saveAsTable(\"Tabela2\") # [nr of buckets, column]\n- from pyspark.sql import storagelevel\n- df.storageLevel #(DISCO, MEMÓRIA, OFFHEAP, SERIALIZADO, REPLICAÇÃO)\n- df.cache()\n- df.persist(storageLevel.DISK_ONLY) # tipo de chache, escolher\n- df.unpersist() # removendo storage level"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26e17a13-3176-485c-8224-1b6fc8330d1c"}}},{"cell_type":"markdown","source":["## KOALAS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24c7c7e7-cdab-47d1-bb15-5be4f16c02b2"}}},{"cell_type":"markdown","source":["- import databricks.koalas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cee1a085-70b9-433b-8f2d-a0432648517d"}}},{"cell_type":"markdown","source":["# Python"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c652375-0ef5-446d-b1b9-f76870f5dd94"}}},{"cell_type":"markdown","source":["## Touples | List | Dic\n- list = [1,2,3,4,5,6,7]\n- list[:3]\n- list.append(2) # Add new line\n- list.insert(0,2) # Altera determinado item\n- list.count(2) #Retorna a quantidade de itens com respectivo valor\n- list(\"ABCD\") == ['A', 'B', 'C', 'D']\n- del(list[1])\n- touples = (1,2,3,4,5,6,7)\n- touples[0]\n- dict = {'1': 'Teste Dados', '2': 2}\n- dict ['1']\n- dict['1'] = 'Teste dados2'\n- del(dict[1])\n- len(dic)\n- dict.keys()\n- dict.values()\n- dict.clear()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ceedf041-75c4-404e-b299-dd6a30574a28"}}},{"cell_type":"markdown","source":["## For | Lambda | Funções\n\n- for n in [list | touples | dict]:\n- for n in range(5,10,2):\n- for item in dict:\n- for item, value in dict.items():\n- string.split(',')\n- del.mutiple(entrada):\n-- var_global = entrada * 3\n-- return var_global = 0\n- x2 = lambda x,y : x+y\n-- x2(2,4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5729266-3681-4379-9c0b-ceefa0183919"}}},{"cell_type":"markdown","source":["## Files\n\n- f = open(filename, 'r'|'w')\n-- data = f.read()\n-- rows = data.split('\\n')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec3f9cbf-e317-4230-a74d-e6f5fdb0f281"}}},{"cell_type":"markdown","source":["# Pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"235d964f-66f0-493c-a1ff-1dcdd91ac426"}}},{"cell_type":"markdown","source":["## File Manipulation \n\n- file = pd.read.csv(filename, sep=',')\n- df = pd.dataframe(file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"047f3411-a40c-4497-99a1-a41f3380a9cd"}}},{"cell_type":"markdown","source":["## Generic Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbcf94e5-79a8-4ffc-97b6-8d797f4b951b"}}},{"cell_type":"markdown","source":["- df.shape\n- df.reshape((2,12)) # Numero de Colunas X Linhas deve ser o mesmo\n- df.dtypes\n- df.head()\n- df.tail()\n- df.describe()\n- df.index\n- df.columns\n- df.values\n- df.to_numpy() # esquecer linhas e colunas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f95bafb-f2f3-4601-ac95-844faf3fdfd2"}}},{"cell_type":"markdown","source":["## Data Frame Manipuation | Reshaping"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a0bde9c-efd3-40ad-a209-08dac3c3edde"}}},{"cell_type":"markdown","source":["- df[0]\n- df[-1]\n- df[:2]\n- df.loc[:5,[\"ColumnA\", \"ColumnB\"]]\n- df.loc[índex1:index2,[\"ColumnA\", \"ColumnB\"]]\n- df.iloc[1] #vem em formato stack\n- df.iloc[2:4, 0:2] # Linha por Coluna\n- df.iloc[[1,5,6],[0,3] # Seleciona as linhas especificadas\n\n\n# Manipulando Colunas | DF\n\n- df['Reais'] = 0 # Criando e/ou alterando valor de uma coluna\n- df['Reais'].str.replace('$','')\n- df['Reais'].str.astype(float)\n- df[['column1','column2']]\n- df['column1'].unique\n- df.drop(index, axis = 0)\n- indexNames = df[ df['Stock'] == 'No' ] # Get names of indexes for which column Stock has value No,\n- -df.drop(indexNames , inplace=True) #.index Delete these row indexes from dataFrame\n- df.dropna(how = 'all') #Apaga linha inteira que esteja vazia\n- df.dropna(how = 'all', axis = 1) #Apaga as colunas que tem registro nulo \n- df.dropna() #Apaga a linha que tem ao menos um registro nulo\n- df.drop_duplicates(subset=\"Id\")\n- datas = pd.date_range('20210101',periods = 6) # periods default diaria, resultado 2021-01-01, 2021-01-02, 2021-01-03, 2021-01-04, 2021-01-05, 2021-01-06\n- datas = pd.date_range('20210101',periods = 6, freq='M')\n- df = pd.DataFrame(np.random.randn(6,4), index = datas, columns = list(\"ABCD\"))\n# Filtros \n\n- df_filtered = df.loc['Departament' == 'procurement']\n- df.loc[df['Department'] == 'PROCUREMENT', ['Name', 'Employee Annual Salary']]\n- df[df.A > 0]\n- df[df > 0] #só traz os valores positivos\n-df[df.col_name.str.startswith('abc').fillna(False)]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50911a80-e44f-4cc2-9da0-139caf15e86c"}}},{"cell_type":"markdown","source":["## Pivot | PivotTable | Stack | Melt | Transporder"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc54383a-a7d6-48fc-8d66-8ed63ae59c58"}}},{"cell_type":"markdown","source":["Pivot\n- df.pivot(index='Dia',columns='Pessoa',values='Gastos')\n\nPivotTable (index is not unique)\n\n- pd.pivot_table(df, index='Data',columns='Vendedor',values='Vendas')\n- pd.pivot_table(df, index='Data',columns='Vendedor',values='Vendas', aggfunc='sum')\n\nStack\n\n- stack_df = df.stack() #torna pilha\n- unstack_df = stack_df.unstack()\n\nMelt\n\n- pd.melt(df, id_vars=['A'], value_vars=['B'])\n- pd.melt(df, id_vars=['A'], value_vars=['B','C'], var_name='VarTeste', value_na,e= 'NomedoValor')\n\nTransporder\n\n- df.T # Transposta, inverte linha por coluna"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1991b1d-bd81-4f26-85bc-56259ce5ff26"}}},{"cell_type":"markdown","source":["## Data Frame Functions\n\n- df[['column1','column2']].sort_values('column2',ascending= False)\n- series = pd.Series([7, 4, 2, np.nan, 6, 9]) # np.nan - numpy lab quando não é possivel identificar algum tipo de dado | series é temporal\n- np.random.randn(6,4) # Gera numero randomico de 6 linhas 4 colunas respeitando variacao de 0 a 1\n- frame = [df_1,df_2,df_3]\n-- framescombinados = pd.concat(frame) \n- grupo = pd.concat([df_1,df_2,df_3], keys=['f1','f2','f3'])\n-- grupo.loc['f2']\n- app1 = df_1.append(df_2).append(df_3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce1182bf-6b90-48c8-8692-4cb30a6fcf05"}}},{"cell_type":"markdown","source":["### Group by \n- df.groupby(['A']).sum()\n- df.groupby(['A', 'B']).sum()\n- df.groupby(['A']).mean()\n- df[['column1','column2']].groupby('column2').sum"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4c1a51a-3748-4bf7-b9fc-afa6335fd6c3"}}},{"cell_type":"markdown","source":["### MERGE \n\nINNER\n- pd.merge(table_esquerda, tbl_direita, on= \"coluna_coincidente\",how=\"left|right|inner|outer\")\n- pd.merge(cadastro_a, cadastro_b[['Id', 'Idade', 'CEP']], on = [\"Id\"], how=\"inner\", suffixes=('_A','_B'))\n\nFULL\n- fulljoin = pd.concat([cadastro_a, cadastro_b], ignore_index=True)\n- fulljoin.drop_duplicates(subset=\"Id\") \n\n\nOUTER (n)\n- pd.merge(cadastro_a, cadastro_b[['Id', 'Idade', 'CEP']], on = [\"Id\"], how=\"outer\", suffixes=('_A','_B'), indicator = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdc140da-879d-4efd-a23d-a09732ce3a5e"}}},{"cell_type":"markdown","source":["## Index\n\n-- Pandas.MultiIndex\n- Levels: Sequence of arrays.\n-- The unique lables for each level\n- Codes: sequence of arrays.\n-- Integers for each level designating which label at each location\n- lables: sequence of arrays\n-- Integers for each level designating which label at each location"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf7630c3-1535-4a1f-b632-7bb48d03a45a"}}},{"cell_type":"markdown","source":["- df[0]\n- df[-1]\n- df[:2]\n\n- pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n- pd.MultiIndex.from_product([numbers, colors], names=['number', 'color'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5f723a4-6ccc-4557-ba21-e7d4bf6bb1e9"}}},{"cell_type":"markdown","source":["# NUMPY"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc0d5f7f-c05d-4b0f-be19-e57b323f0c5b"}}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52d77608-f227-45d7-a6a0-e2473bf135c0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["- np.random.randn(8)\n- np.size(df.values)\n- np.random.choice(Lista)\n- np.round(float,2)\n- "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a867f175-a90b-4cf1-9576-ae3734c69fbc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: array([-0.04386612,  1.18549393,  0.77710969,  0.89557391,  1.22138275,\n        0.54214681, -0.65052194, -0.30737109])","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: array([-0.04386612,  1.18549393,  0.77710969,  0.89557391,  1.22138275,\n        0.54214681, -0.65052194, -0.30737109])"]}}],"execution_count":0},{"cell_type":"markdown","source":["# SQL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"686c3f8a-c4ed-4cd3-a9e7-0edbd32f8999"}}},{"cell_type":"markdown","source":["## CTE | Queries\n\nWITH\n    cteReports (EmpID, FirstName, LastName, MgrID, EmpLevel)\n      AS\n    (\n      SELECT EmployeeID, FirstName, LastName, ManagerID, 1\n      FROM Employees\n      WHERE ManagerID IS NULL\n      UNION ALL\n      SELECT e.EmployeeID, e.FirstName, e.LastName, e.ManagerID, \n        r.EmpLevel + 1\n      FROM Employees e\n        INNER JOIN cteReports r\n          ON e.ManagerID = r.EmpID\n      )\n      SELECT\n      FirstName + ' ' + LastName AS FullName, \n      EmpLevel,\n      (SELECT FirstName + ' ' + LastName FROM Employees \n      WHERE EmployeeID = cteReports.MgrID) AS Manager\n      FROM cteReports \n      ORDER BY EmpLevel, MgrID"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"feeabade-30f8-4403-b08d-eeeefdb3576a"}}},{"cell_type":"markdown","source":["## Funções de Classificação :: ROW | RANK | DENSE RANK\n\n- RANK - Agrupa e ID de forma não sequencial 1, 1, 3, 3, 4\n      select rank() over (order by estado asc) as rank_uf, regiao, estado from regiao ::  SP 1, AC 2, RJ 3\n      select rank() over (order by regiao asc) as rank_uf, regiao, estado from regiao :: Resultado Centro Oeste 1 Centro Oeste 1 SP 3\n    \n- DENSE_RANK - Agrupa e ID de forma sequencial 1, 2, 3, 4\n      select dense_rank() over (order by estado asc) as rank_uf, regiao, estado from regiao :: \n      select dense_rank() over (order by regiao asc) as rank_uf, regiao, estado from regiao :: Resultado Centro Oeste 1 Centro Oeste 1 SP 2\n\n- ROW_Number - Conta sequencial\n      select row_number() over (order by estado asc) as rank_uf, regiao, estado from regiao ::  SP 1, AC 2, RJ 3\n      select row_number() over (order by regiao asc) as rank_uf, regiao, estado from regiao :: Resultado Centro Oeste 1 Centro Oeste 1 SP 2\n\n\n- ntitle\n      select ntitle(3) over (order by regiao asc) as rank_uf, regiao, estado from regiao :: Dividi em blocos o que foi parametrizado"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7299c8d-976d-42a4-9f8c-6c5a165d17b3"}}},{"cell_type":"markdown","source":["## Queries e operações lógicas\n\n- LIKE\n      select * from table where colunas like '[CS]he%'\n      select * from table where colunas like 'he[CS]%'\n\n- SubQueries (Exists | IN)\n\n      select * from table a Where exists (select * from table b where a.id = b.id)\n              \n      select * from table a Where a.id in (select b.id from table b)\n  \n      select a, b, c, (select avg(columna) from table B where a.id = b.idav) from table a \n\n- Update | Delete com com subselect\n      update table a set columna = (select count(*) as columna from table b)\n      delete table A where A.ID in (select B.ID from table B)\n\n- Alter Table\n      alter table tableA add|dropp column A (for add should inform the type)\n      \n- Choose \n      select choose(mes, 'winter', 'winter', 'spring', 'spring', 'spring', 'spring', 'summer', 'summer', 'summer', 'summer')\n\n- IIF \n      select IIF(A > B, 'Maior', 'Menor')\n\n- IF\n      if 1 = 1 begin print \"Teste\" end else \"Teste2\"\n      \n- While\n      While @var <= 10 begin set @var += 1 if @var = 5 break else continue end\n      \n- nullif | isnull\n      isnull(val1/nullif(val2,0),0)\n\n- coalesce :: Retorna o primeiro valor não nulo\n      colalesce(null, null, 'TerceiraColuna)\n\n- Output modes (Insert | Deleted)\n      update tablea set columna = (CASE WHEN columna >= 10 then 1 else 0 end)\n      output deleted.columna, \n      deleted.columna as antes,\n      inserted.columna as depois \n      Where idn=0\n      \n- CTE\n      WITH cteReports (EmpID, FirstName, LastName, MgrID, EmpLevel)\n      AS (\n      SELECT EmployeeID, FirstName, LastName, ManagerID, 1\n      FROM Employees WHERE ManagerID IS NULL\n      )\n      SELECT\n      FirstName + ' ' + LastName AS FullName, \n      EmpLevel, (SELECT FirstName + ' ' + LastName FROM Employees WHERE EmployeeID = cteReports.MgrID) AS Manager\n      FROM cteReports \n      ORDER BY EmpLevel, MgrID\n      \n- Begin Try (Trativa de erros)\n      Begin try 1/0 end try\n      Begin catch select \n        error_number(),\n        error_severety(),\n        error_state(),\n        error_procedure(),\n        error_line(),\n        error_message()\n      end catch\n      \n- Cursor\n      declare meu_cursor\n      cursor local for select columna from tablea\n      open meu_cursor\n        fecth next from meu_cursor into @variavel\n        while (@@FETCH_STATUS = 0)\n          begin\n            print @variavel\n            fetch next from meu_cursos into @variavel\n          end\n        close meu_cursor\n        deallocate meu_cursor\n        \n        \n- Cursor (Comandos para navegar dentro do cursor)\n\n      - FETCH ABSOLUTE 1 FROM CURSOR; :: Go directly to the line\n      - FETCH NEXT FROM CURSOR;\n      - FETCH LAST FROM CURSOR;\n      - FETCH PRIOR FROM CURSOR;\n      - FETCH RELATIVE 3 FROM CURSOR; :: AVANÇA TRÊS POSIÇÕES\n      \n      \n- Trigger\n      create trigger trg1\n      on Salario\n        after update\n      as \n      begin\n      -- code\n      end\n\n- COLLATE\n      create table (pessoa) (\n        ID_PESSOA INTEGER,\n        NOME VARCHAR(100) COLLATE WIN_PTBR)\n\n- INDENTITY\n      SET IDENTITY_INSERT TABLEA ON INSERT INTO TABLEA ........\n      SET IDENTITY_INSERT TABLEA OFF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"077f3283-b4f7-4688-b381-f4f48a9dd7b6"}}},{"cell_type":"markdown","source":["## Conversões | Funções Chart\n- CAST | TRY_CAST\n      select CAST(column as decimal(5,2))\n      select TRY_CAST(column as decimal(5,2))\n\n- CONVERT | TRY_CONVERT\n      select convert(decimal(5,2), column)\n      select TRY_CONVERT(decimal(5,2), colunnb)\n    \n- PARSE | TRY_PARSE\n      select TRY_PARSE('Monday, 13 December 2010' as datetime USING 'en-US') :: 2010/12/13\n      select TRY_PARSE('Segunda-feira, 13 Dezembro 2010' as datetime USING 'pt-BR') :: 13/12/2010\n      select TRY_PARSE('R$10,00' as money USING 'pt-BR') :: 10.00\n      select TRY_PARSE('$10.00' as money USING 'en-US') :: 10.00\n      set language 'English'; select TRY_PARSE('12/20/2020' as datetime) \n      set language 'Português'; select TRY_PARSE('2020/12/10' as datetime)\n\n- set dateformat dmy (DD-MM-YYYY)\n\n# CHAR\n\n- LTRIM :: Remove espaçoes à esquerda\n        select ltrim(@string)\n        \n- RTRIM :: Remove espaçoes à direita\n        select rtrim(@string)\n        \n- STR  :: Retorna char convertidos de dados numericos\n        select str(@number, 5, 2)\n        \n- CONCAT :: Concantena\n        select CONCAT(@string, @string2)\n        \n- CONCAT_WS :: Separa e retorna valores de cadeia de caracteres concatenados com delimitador\n        select CONCAT_WS('|',@string, @string2)\n    \n- Replace :: Substitui\n        select replace(@string, 'a', 'b')\n\n- LEFT :: Retorna parte esquerda de um char\n\n        select left(@string,5)\n- RIGHT :: Retorna parte esquerda de um char\n\n        select right(@string,5)\n- ISNULL :: Trata quand for branco\n\n        select isnull(columna, '')\n        \n# DATE\n\n- Datename(month, a.nascimento) :: Retorna o numero do mes\n- Datediff(day, date1, date2)\n- DateAFF(Month, 2, getdate())\n- convert(varchar(10),getdate(),120|103|100|1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76743c08-eff9-47cc-abe1-023a8e6ec4e5"}}},{"cell_type":"markdown","source":["##CONCEITOS BASICOS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9695c911-2577-47a1-992e-6832258a61a2"}}},{"cell_type":"markdown","source":["# Star Schema | SnowFlake\nNo modelo Star, todas as tabelas de dimensões necessárias têm apenas chaves estrangeiras nas tabelas de fatos. O modelo Snowflake possui mais junções entre a tabela de dimensões e a tabela de fatos, portanto\n\n# Indices\n      - Clusterizados: é montado na própria tabela, criando a estrutura ordenada de árvore para facilitar as buscas. Por este motuvo, apenas 1 indice desse tipo pode ser criado por tabela e não pode utilizar INCLUDE de colunas neste tipo de indice.\n      - Índice NonClustered: é uma estrutura ORDENADA à parte que contém apenas. pode-se ter mais de um, e importante que o indice clustered automaticamente também esta disponivel para o indice cluster.\n      - Lookup: Quando o usuário pede uma informação que não esta no indice cluster e no indice nocluster, e com isso ele baseado no indice cluster ele tem que fazer um LOOKUP para pegar os registros faltantes\n      - SEEK: Quando na consulta do indice NoCluster estão todos os dados disponiveis (NoCluster + Cluster)\n      - SCAN: Lê a tabela toda\n      - HEAP: Tabelas sem indices\n      \n# Collate\n      - Tratar case sensentive | ascentos\n      - \n\n# Tipo de Dados\n- Char: tamanho fixo máximo de 8k char\n- varcha(n): tamanho variavel máximo 8k char\n- varchar(max): tamanho máximo de 1073741824 char\n- text: variável máximo 2gigas de texto\n- nChar: tamanho fixo máximo de 8k char UNICODE permitir maior flexibilidade nos dados pra aceitar char especiais como mandarim, japones e etc\n- nvarcha(n): tamanho variavel máximo 8k char UNICODE\n- nvarchar(max): tamanho máximo de 1073741824 char UNICODE\n- ntext: variável máximo 2gigas de texto UNICODE\n- Image\n- Date\n- DateTime\n- time\n- bit (0, 1 or null)\n- decimal\n- int\n- money\n- numeric\n\n# Constraints\n- not null\n- unique - Garante que os valores em uma coluna sejam diferentes\n- primary key\n- foreign key\n- default - define um valor padrão para uma coluna quando nenhum valor é especificado\n- index \n- check - Valida valor que inserido em uma coluna, como uma restrição. Estoque por exemplo, reestringir < 0 | ou data de nascimento no futuro por exemplos\n\n# Union and Union ALL\n- Union resultado com distict junta as tabelas com a mesma estrutura de colunas e tipo de dados\n- Uniona ll resultado com distict junta as tabelas com a mesma estrutura de colunas e tipo de dados\n\n# Tabelas temporárias\n- Temp Table locais #\n- Temp Table globais ## (conexões ativas)\n\n# Funções\n- Funções em T-SQL são rotinars que retornam valores ou tabelas\n      - Escalares: Definidas pelo usuário retornam um valor único de dados do tipo definido na cláusula return;\n      - Tabela: Retorna um tipo de dado table\n      - Em linha (In-Line): Muito utilizadas para parametrizar viewws. \n      \n# Triggers\n      - FOR: Junto com a ação\n      - AFTER: Após a ação\n      - INSTEAD OF: Rodar invés\n      \n# 3FN\n\n      - Elimina atributos não chaves que dependem de outros atributos não chaves, por exemplo: NF, Cod_Vendedor, Nome_Vendedor. \n      Para aplicar a terceira forma normal, transformar o Cod_Vendedor como FK\n  \n# Orientada objeto\nClasse: De outra forma, uma classe pode ser definida como uma descrição das propriedades ou estados possíveis de um conjunto de objetos, bem como os comportamentos ou ações aplicáveis a estes mesmos objetos.\nHeranca: Herança é um princípio de orientação a objetos, que permite que classes compartilhem atributos e métodos, através de \"heranças\". Ela é usada na intenção de reaproveitar código ou comportamento generalizado ou especializar operações ou atributos. O conceito de herança de várias classes é conhecido como herança múltipla. Como exemplo pode-se observar as classes 'aluno' e 'professor', onde ambas possuem atributos como nome, endereço e telefone."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73dc9374-5587-4f73-8ba3-a66ff964c7c4"}}},{"cell_type":"markdown","source":["# Career"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c902646-9088-4036-9e00-b7cfe68b6b09"}}},{"cell_type":"markdown","source":["Graduaçõa\nMBA\n\tTempo de experiencia\nInicio (GeneXus rodando no banco de dados SQL e DB2)\n- Nessa função tive oportunidade de trabalhar em duas consultorias como outsourcing e em uma agencia de turismo\n\nEm 2012 recebi uma oferta da Nestlé para trabalhar como desenvolvedor GeneXus e PL/SQL. \n- Atender a area de sistemas legados Marketing & Sales e Customer Services\n- Alguns dos sistemas eram in-house e outros a gente fazia a ponte entre fornecedor e cliente\n- Contato mais de perto com o banco de dados, desenvolvimento\n- In this position I also have different roles like work as project manager, I supported the SAP team in HANA SAP Migration.. so it was a time where I could leverage in general my knowledge and consequently my skills\n\nEm 2018\n- História do stream de bigdata\n- Virou a chavinha pra trabalhar com o Cloud\n- Contar sobre os projetos\n    - Sell in X Sell out\n        - Scope: integrated how much the customers were selling and cross how much we are selling\n        - Responsabilidades e tecnologias\n        - Data Lake\n        - Processo de ETL com Data Factory, Databricks, spark, pyspark, DW, PowerBi\n    - Cockpit Magento\n    - Comparação entre acordos comerciais X execução \n\nEm 2021 - Exceedra\n- Reestruturação \n- Entender a necessidade do cliente , fazer treinamentos básicos de PowerBi, explicar os dados e origem dos dados\n- Criação de queries para fazer o calculo da loja perfeita"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de09b551-9190-4748-94e2-8759b55ab5a5"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48584b9b-e29f-435d-ac15-ceac7bb11436"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Main Commands","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3701362407691266}},"nbformat":4,"nbformat_minor":0}
